# ðŸ§  ReadWriteRun

Welcome to **ReadWriteRun**, a structured, hands-on repository where I dive deep into impactful machine learning research papers â€” not just to read, but to **rebuild, understand, and apply** them.

This project has three core tracks:
- **Applications**: End-to-end implementations of applied research papers
- **Architectures**: Recreating foundational models from scratch or with libraries like PyTorch
- **Blog Experiments**: Reproducing and extending interesting blog posts and code walkthroughs

---

## ðŸ“¦ Applications
- [Text2SQL](./01-applications/text2sql/)
- [Chatbot Summarizer](./01-applications/chatbot-summarizer/)

---

## ðŸ§± Architectures

### ðŸ”¹ Fully Connected Networks
- [] [Perceptron](./02-architectures/perceptron/)
- [] [MLP (Multi-Layer Perceptron)](./02-architectures/mlp/)
- [ ] [Backpropagation](./02-architectures/backpropagation/)

### ðŸ”¹ Convolutional Neural Networks (CNN)
- [ ] [Basic CNN](./02-architectures/basic-cnn/)
- [ ] [LeNet-5](./02-architectures/lenet/)
- [ ] [AlexNet](./02-architectures/alexnet/)
- [ ] [VGGNet](./02-architectures/vgg/)
- [ ] [Inception](./02-architectures/inception/)
- [ ] [ResNet](./02-architectures/resnet/)
- [ ] [MobileNet / EfficientNet](./02-architectures/mobilenet/)

### ðŸ”¹ Recurrent Neural Networks (RNN)
- [ ] [Vanilla RNN](./02-architectures/rnn/)
- [ ] [LSTM](./02-architectures/lstm/)
- [ ] [GRU](./02-architectures/gru/)
- [ ] [Bidirectional RNN](./02-architectures/birnn/)
- [ ] [Stacked RNNs](./02-architectures/stacked-rnn/)

### ðŸ”¹ Word Embeddings
- [x] [Word2Vec](./02-architectures/word2vec/)
- [ ] [GloVe](./02-architectures/glove/)
- [ ] [FastText](./02-architectures/fasttext/)

### ðŸ”¹ Autoencoders
- [ ] [Basic Autoencoder](./02-architectures/autoencoder/)
- [ ] [Denoising Autoencoder](./02-architectures/denoising-autoencoder/)
- [ ] [Sparse Autoencoder](./02-architectures/sparse-autoencoder/)
- [ ] [Variational Autoencoder (VAE)](./02-architectures/vae/)

### ðŸ”¹ Attention & Transformers
- [ ] [Scaled Dot Product Attention](./02-architectures/attention-scaled-dot/)
- [ ] [Multi-Head Attention](./02-architectures/attention-multihead/)
- [ ] [Positional Encoding](./02-architectures/positional-encoding/)
- [ ] [Transformer Encoder](./02-architectures/transformer/)
- [ ] [Transformer Decoder](./02-architectures/transformer-decoder/)
- [ ] [Full Transformer](./02-architectures/transformer-full/)
- [ ] [BERT, GPT-2/3, T5 (HF)](./02-architectures/bert-gpt/)

### ðŸ”¹ Generative Models
- [ ] [Vanilla GAN](./02-architectures/gan/)
- [ ] [DCGAN](./02-architectures/dcgan/)
- [ ] [Conditional GAN](./02-architectures/cgan/)
- [ ] [CycleGAN](./02-architectures/cyclegan/)
- [ ] [StyleGAN](./02-architectures/stylegan/)
- [ ] [Diffusion Models](./02-architectures/diffusion/)

### ðŸ”¹ Reinforcement Learning
- [ ] [Q-Learning](./02-architectures/qlearning/)
- [ ] [DQN](./02-architectures/dqn/)
- [ ] [Policy Gradient](./02-architectures/policy-gradient/)
- [ ] [Actor-Critic](./02-architectures/actor-critic/)
- [ ] [PPO / A3C](./02-architectures/ppo/)

### ðŸ”¹ Training Tricks
- [ ] [Dropout](./02-architectures/dropout/)
- [ ] [BatchNorm / LayerNorm](./02-architectures/batchnorm/)
- [ ] [Residual Connections](./02-architectures/residual/)
- [ ] [Gradient Clipping](./02-architectures/gradclip/)
- [ ] [Weight Initialization](./02-architectures/init-xavier-he/)

---

## ðŸ§ª Blog Experiments
- [Quantization in Machine Learning](./03-blog-experiments/quantization_in_ml/)


