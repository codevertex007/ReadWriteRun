{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c965b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\journey_2025\\ReadWriteRun\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c62e935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\journey_2025\\ReadWriteRun\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\91875\\.cache\\huggingface\\hub\\datasets--wikitext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 123598.78 examples/s]\n",
      "Generating train split: 100%|██████████| 1801350/1801350 [00:02<00:00, 620522.10 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 458433.85 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89568cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "849d8c98",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'param_groups'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(lr_scheduler)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lr_scheduler\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43mget_lr_scheduler\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43madam\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mget_lr_scheduler\u001b[39m\u001b[34m(optimizer, total_epochs, verbose)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mScheduler to linearly decrease learning rate, \u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03mso thatlearning rate after the last epoch is 0.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m lr_lambda = \u001b[38;5;28;01mlambda\u001b[39;00m epoch: (total_epochs - epoch) / total_epochs\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m lr_scheduler = \u001b[43mLambdaLR\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_lambda\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr_lambda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(lr_scheduler)\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m lr_scheduler\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\journey_2025\\ReadWriteRun\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:279\u001b[39m, in \u001b[36mLambdaLR.__init__\u001b[39m\u001b[34m(self, optimizer, lr_lambda, last_epoch)\u001b[39m\n\u001b[32m    277\u001b[39m \u001b[38;5;28mself\u001b[39m.lr_lambdas: \u001b[38;5;28mlist\u001b[39m[Callable[[\u001b[38;5;28mint\u001b[39m], \u001b[38;5;28mfloat\u001b[39m]]\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr_lambda, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(lr_lambda, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28mself\u001b[39m.lr_lambdas = [lr_lambda] * \u001b[38;5;28mlen\u001b[39m(\u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparam_groups\u001b[49m)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lr_lambda) != \u001b[38;5;28mlen\u001b[39m(optimizer.param_groups):\n",
      "\u001b[31mAttributeError\u001b[39m: 'str' object has no attribute 'param_groups'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# -- 1. Tokenizer & dataset loader --------------------------------\n",
    "\n",
    "def get_english_tokenizer() -> Callable[[str], List[str]]:\n",
    "    return get_tokenizer(\"basic_english\", language=\"en\")\n",
    "\n",
    "\n",
    "def get_data_iterator(\n",
    "    ds_name: str,\n",
    "    split: str,\n",
    "    root: str\n",
    "):\n",
    "    if ds_name == \"WikiText2\":\n",
    "        data = WikiText2(root=root, split=split)\n",
    "    elif ds_name == \"WikiText103\":\n",
    "        data = WikiText103(root=root, split=split)\n",
    "    else:\n",
    "        raise ValueError(\"Choose dataset from: WikiText2, WikiText103\")\n",
    "    from torchtext.data import to_map_style_dataset\n",
    "    return to_map_style_dataset(data)\n",
    "\n",
    "# -- 2. Vocabulary builder -----------------------------------------\n",
    "\n",
    "def build_vocab_from_corpus(\n",
    "    data_iter,\n",
    "    tokenizer: Callable[[str], List[str]],\n",
    "    min_freq: int = MIN_WORD_FREQUENCY,\n",
    "    specials: List[str] = ['<unk>']\n",
    "):\n",
    "    vocab = build_vocab_from_iterator(\n",
    "        map(tokenizer, data_iter),\n",
    "        specials=specials,\n",
    "        min_freq=min_freq\n",
    "    )\n",
    "    vocab.set_default_index(vocab['<unk>'])\n",
    "    return vocab\n",
    "\n",
    "# -- 3. Pair generation helper ------------------------------------\n",
    "\n",
    "def generate_pairs(\n",
    "    token_ids: List[int],\n",
    "    window: int,\n",
    "    model: str\n",
    ") -> List[Tuple[List[int], int]]:\n",
    "    pairs = []\n",
    "    L = len(token_ids)\n",
    "    for idx, center in enumerate(token_ids):\n",
    "        start = max(0, idx - window)\n",
    "        end = min(L, idx + window + 1)\n",
    "        context = token_ids[start:idx] + token_ids[idx+1:end]\n",
    "\n",
    "        if model == 'cbow':\n",
    "            if context:\n",
    "                pairs.append((context, center))\n",
    "        else:\n",
    "            for ctx in context:\n",
    "                pairs.append((center, ctx))\n",
    "    return pairs\n",
    "\n",
    "# -- 4. Unified collate with PyTorch utilities --------------------\n",
    "\n",
    "def collate_word2vec(\n",
    "    batch: List[str],\n",
    "    text_pipeline: Callable[[str], List[int]],\n",
    "    window: int,\n",
    "    model: str\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # tokenize and convert to IDs\n",
    "    token_ids_batch = [torch.tensor(text_pipeline(text), dtype=torch.long)\n",
    "                       for text in batch]\n",
    "    # generate all pairs\n",
    "    inp_list, tgt_list = [], []\n",
    "    for ids in token_ids_batch:\n",
    "        if len(ids) < window * 2 + 1:\n",
    "            continue\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            ids = ids[:MAX_SEQUENCE_LENGTH]\n",
    "        for inp, tgt in generate_pairs(ids.tolist(), window, model):\n",
    "            inp_list.append(torch.tensor(inp, dtype=torch.long))\n",
    "            tgt_list.append(tgt)\n",
    "    if model == 'cbow':\n",
    "        # pad context sequences\n",
    "        padded = pad_sequence(inp_list, batch_first=True,\n",
    "                              padding_value=text_pipeline.vocab['<unk>'])\n",
    "        return padded, torch.tensor(tgt_list, dtype=torch.long)\n",
    "    else:\n",
    "        centers = torch.tensor(inp_list, dtype=torch.long)\n",
    "        targets = torch.tensor(tgt_list, dtype=torch.long)\n",
    "        return centers, targets\n",
    "\n",
    "# -- 5. High-level dataloader + vocab getter ----------------------\n",
    "\n",
    "def get_dataloader_and_vocab(\n",
    "    model_name: str,\n",
    "    ds_name: str,\n",
    "    ds_type: str,\n",
    "    data_dir: str,\n",
    "    batch_size: int,\n",
    "    shuffle: bool,\n",
    "    vocab: Optional[object] = None\n",
    "):\n",
    "    data_iter = get_data_iterator(ds_name, ds_type, data_dir)\n",
    "    tokenizer = get_english_tokenizer()\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = build_vocab_from_corpus(data_iter, tokenizer)\n",
    "\n",
    "    # build a transform pipeline\n",
    "    text_pipeline = Sequential(\n",
    "        tokenizer,\n",
    "        VocabTransform(vocab),\n",
    "        TruncateTransform(MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "    # stash vocab for padding\n",
    "    text_pipeline.vocab = vocab\n",
    "\n",
    "    window = CBOW_N_WORDS if model_name == 'cbow' else SKIPGRAM_N_WORDS\n",
    "    collate_fn = partial(\n",
    "        collate_word2vec,\n",
    "        text_pipeline=text_pipeline,\n",
    "        window=window,\n",
    "        model=model_name\n",
    "    )\n",
    "\n",
    "    loader = DataLoader(\n",
    "        data_iter,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    return loader, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e37fc4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86b480f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    [\"the\", \"sky\", \"is\", \"blue\"],\n",
    "    [\"the\", \"sun\", \"is\", \"bright\"],\n",
    "    [\"we\", \"love\", \"natural\", \"language\", \"processing\"],\n",
    "    [\"word2vec\", \"learns\", \"word\", \"embeddings\"],\n",
    "    [\"pytorch\", \"is\", \"great\", \"for\", \"deep\", \"learning\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee9cbb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "idx2word = {}\n",
    "idx = 0\n",
    "\n",
    "for c in corpus:\n",
    "    for word in c:\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = idx\n",
    "            idx2word[idx] = word\n",
    "            idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "686bef8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_pairs(corpus, window_size=2, model_type=\"cbow\"):\n",
    "    training_pairs = []\n",
    "\n",
    "    for sentence in corpus:\n",
    "        for idx in range(len(sentence)):\n",
    "            center_word = sentence[idx]\n",
    "            center_word_idx = word2idx[center_word]\n",
    "\n",
    "            # Get left and right context\n",
    "            start = max(0, idx - window_size)\n",
    "            end = min(len(sentence), idx + window_size + 1)\n",
    "\n",
    "            context = sentence[start:idx] + sentence[idx+1:end]\n",
    "            context_idx = [word2idx[word] for word in context]\n",
    "\n",
    "            if model_type == \"cbow\":\n",
    "                if context: \n",
    "                    training_pairs.append((context_idx, center_word_idx))\n",
    "            else:\n",
    "                for context_word in context:\n",
    "                    context_word_idx = word2idx[context_word]\n",
    "                    training_pairs.append((center_word_idx, context_word_idx))\n",
    "\n",
    "    return training_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "998369d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([1, 2], 0), ([0, 2, 3], 1), ([0, 1, 3], 2), ([1, 2], 3), ([4, 2], 0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs = get_training_pairs(corpus, window_size=2, model_type=\"cbow\")\n",
    "training_pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "580001a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2vecModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward_cbow(self, X):  # X: (batch_size, context_size)\n",
    "        embedded = self.embedding(X)        # (batch_size, context_size, emb_dim)\n",
    "        averaged = torch.mean(embedded, dim=1)  # (batch_size, emb_dim)\n",
    "        return self.linear(averaged)        # (batch_size, vocab_size)\n",
    "\n",
    "    def forward_skipgram(self, X):  # X: (batch_size,)\n",
    "        embedded = self.embedding(X)        # (batch_size, emb_dim)\n",
    "        return self.linear(embedded)        # (batch_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a34f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
