
# Word2Vec from Scratch (CBOW & Skip-Gram)

This project implements the **Word2Vec** algorithm from scratch using PyTorch. Both **CBOW** and **Skip-Gram** architectures are supported.

---

## ğŸ§  Supported Architectures

| Model     | Objective                                      |
|-----------|-----------------------------------------------|
| Skip-Gram | Predict surrounding context words from center |
| CBOW      | Predict center word from surrounding context  |

---

## ğŸ“ Project Structure

```

word2vec/
â”œâ”€â”€ data/                             
â”œâ”€â”€ notebooks/                        # Notebooks only for inference and t-SNE plots
â”‚   â”œâ”€â”€ 01\_skipgram\_visualization.ipynb
â”‚   â””â”€â”€ 02\_cbow\_visualization.ipynb
â”œâ”€â”€ utils/                            # Core code
â”‚   â”œâ”€â”€ cbow\_model.py                 # CBOW implementation
â”‚   â”œâ”€â”€ skipgram\_model.py             # Skip-Gram implementation
â”‚   â”œâ”€â”€ dataloader.py                 # dataloader
â”‚   â”œâ”€â”€ trainer.py                    # Training class
â”‚   â”œâ”€â”€ helper.py                     # Save/load vocab, config, etc.
â”‚   â””â”€â”€ constants.py                  # Global constants
â”œâ”€â”€ weights/                          # Saved models, loss logs, and vocab
â”‚   â””â”€â”€ cbow\_wikitext-103-v1/
â”‚       â”œâ”€â”€ model.pth
â”‚       â”œâ”€â”€ vocab.pt
â”‚       â”œâ”€â”€ loss.json
â”‚       â””â”€â”€ config.yaml
â”œâ”€â”€ config.yaml                       # Training configuration
â”œâ”€â”€ train.py                          
â””â”€â”€ README.md                         # You're here

````

---

## ğŸ”§ How to Train

Edit `config.yaml` to select model and hyperparameters:

```yaml
model_name: cbow                     # or skipgram
data_dir: data/wikitext-103-v1
train_batch_size: 96
val_batch_size: 96
embedding_dim: 100
optimizer: Adam
learning_rate: 0.01
epochs: 10
model_dir: weights/cbow_wikitext-103-v1
````

Run training:

```bash
python train.py --config config.yaml
```

Artifacts saved in `{model_dir}`:

* `model.pth`: Trained embedding model
* `vocab.pt`: Word-to-index mapping
* `loss.json`: Training/validation loss history
* `config.yaml`: Copy of config used for run

---

## ğŸ“Š Embedding Visualization (t-SNE)

Use notebooks to visualize word embeddings in 2D using t-SNE:

* `notebooks/01_skipgram_visualization.ipynb`
* `notebooks/02_cbow_visualization.ipynb`

---


## âœ… TODO

* [x] CBOW model
* [x] Skip-Gram model
* [x] t-SNE word embedding visualization
* [x] Word analogy evaluation (e.g., king - man + woman â‰ˆ queen)
* [ ] Negative sampling

---